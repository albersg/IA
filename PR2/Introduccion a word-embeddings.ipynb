{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifrZAoRXQfNW"
   },
   "source": [
    "# Manejo de word-embeddings con Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvOimK2-z5ei"
   },
   "source": [
    "[Gensim](https://radimrehurek.com/gensim/intro.html#what-is-gensim) es una biblioteca gratuita de Python de código abierto para representar documentos como vectores semánticos. \n",
    "\n",
    "Entre otras cosas incluye los algoritmos de Word2Vec y FastText, y funcionalidades para manejar las word-embeddings resultantes.\n",
    "\n",
    "Nosotros vamos a utilizarlo para cargar la matriz [word-embedding generada por Glove](https://nlp.stanford.edu/data/glove.6B.zip) que se descarga de [la web del proyecto](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "Del archivo comprimido que nos descargamos tomaremos solamente el fichero más pequeño con el espacio vectorial de 50 dimensiones `glove.6B.50d.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG4dNAYCQfNa"
   },
   "source": [
    "# Carga del fichero\n",
    "Antes de cargar el fichero de textocon la word-embedding de Glove es necesario traducir el fichero al formato word2vec, que reconoce Gensim, que en realidad es prácticamente idéntico al que ya tenemos.\n",
    "\n",
    "Dado que los vectores de palabras (las word-embeddings) son independientes del algoritmo que se usó para obtenerlos (Word2Vec, FastText, VarEmbed, etc.), pueden representarse mediante una estructura independiente. En concreto Gensim los implementa a través de una estructura que se llama `KeyedVectors` y que es esencialmente un mapeo clave-valor entre cada palabra y su vector correspondiente [con otras funcionalidades adicionales](https://radimrehurek.com/gensim/models/keyedvectors.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nG6CRBObTHJu"
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Traducimos el fichero glove a un fichero con formato word2vec\n",
    "glove_file = 'glove.6B.50d.txt'\n",
    "#glove_file = '/content/drive/MyDrive/Asignaturas/IA/IA2_21-22/Material/PLN/tmp/practica/Parte 1/glove.6B.50d.txt'\n",
    "\n",
    "# Gensim 3 requiere hacerlo en varios pasos\n",
    "#word2vec_file = 'glove.6B.50d.txt.word2vec'\n",
    "#glove2word2vec(glove_file, word2vec_file)\n",
    "#model = KeyedVectors.load_word2vec_format(word2vec_file, binary=False)\n",
    "\n",
    "# Gensim 4 puede transformar de glove a word2vec directamente\n",
    "model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBGbbqr13uCX"
   },
   "source": [
    "# Manipulación básica del word-embedding\n",
    "\n",
    "La clase `KeyedVectors` nos permite hacer muchas casos. \n",
    "\n",
    "Lo que hace principalmente es mapear cada palabra con su vector correspondiente (de string a array 1D numpy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1647544270803,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "U21X3rPTQDnc",
    "outputId": "014fefe3-e552-4f92-88a7-336c18cef370"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28704  , -0.60904  , -0.74821  ,  0.17686  ,  0.92118  ,\n",
       "        0.36994  ,  0.10464  , -1.0945   ,  0.45246  ,  0.75508  ,\n",
       "        0.041291 ,  0.61758  ,  0.82508  ,  0.7044   ,  0.12365  ,\n",
       "       -0.081073 , -0.060157 ,  0.90245  , -1.2025   ,  0.04606  ,\n",
       "       -0.57476  , -0.15483  ,  0.45276  , -0.087841 ,  0.49575  ,\n",
       "       -1.0462   , -0.59215  ,  0.40345  , -0.11113  , -1.3799   ,\n",
       "        0.97611  , -0.312    , -0.75014  ,  0.44309  , -0.0056967,\n",
       "        0.44116  ,  0.073432 , -0.76681  ,  0.12938  , -0.54472  ,\n",
       "       -0.74424  ,  0.19052  , -0.80318  ,  0.54953  ,  1.1741   ,\n",
       "        0.37876  ,  0.93837  , -1.0941   ,  0.2474   , -0.69492  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El uso básico sería darle una palabra y que nos devuelva su vector correspondiente\n",
    "model['monkey']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8iH8vPD7yms"
   },
   "source": [
    "## Encontrando palabras similares a otras\n",
    "Experimentar y ser inquisitivo con los datos siempre es una buena manera de tomar contacto con ellos para empezar a entenderlos.\n",
    "\n",
    "En este caso, podemos jugar a encontrar palabras similares en el espacio de 50 dimensiones de la word-embedding. Esto nos hará entender mejor la similitud \"semántica\" que de facto ha quedado reflejada.\n",
    "\n",
    "Con `most_similar` se nos devolverán las palabras más similares de acuerdo a la similitud del coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1647521099077,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "mWH78F4MtfW7",
    "outputId": "f83cd817-a360-4322-ff78-94b0e66e6c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yellow', 0.8995459079742432), ('blue', 0.8901659250259399), ('green', 0.8561933040618896), ('black', 0.8400583863258362), ('purple', 0.8323202133178711)]\n"
     ]
    }
   ],
   "source": [
    "x = model.most_similar(positive=['red'], topn=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1647521358709,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "PORZvmKf6pll",
    "outputId": "9aeb2398-08ff-4ec6-fcab-a89b967b767e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('scissors', 0.7837907671928406), ('sharpener', 0.7752447128295898), ('pencils', 0.7656055688858032), ('crayon', 0.738135039806366), ('sharpeners', 0.7206112146377563)]\n"
     ]
    }
   ],
   "source": [
    "# Al buscar un término similar a \"pencil\" encontramos otros objetos de papelería básicos, pero también sus plurales\n",
    "x = model.most_similar(positive=['pencil'], topn=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1647521108046,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "sABAlRxB6E3Y",
    "outputId": "005d681b-2bf1-466e-8ddf-8cb3fbf4c9f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('milan', 0.8963416218757629), ('barcelona', 0.8910947442054749), ('valencia', 0.8619170188903809), ('sevilla', 0.786338746547699), ('atletico', 0.783437192440033)]\n"
     ]
    }
   ],
   "source": [
    "# Al buscar un término similar a \"madrid\" encontramos ciudades españolas pero también europeas, \n",
    "# e incluso la palabra \"atlético\" asociada por el equipo de fútbol\n",
    "x = model.most_similar(positive=['madrid'], topn=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1647522524249,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "42j8L_V96TMK",
    "outputId": "7d83ee37-ddd1-4754-d58f-88981049adb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spider-man', 0.6691319942474365), ('mortal', 0.648215651512146), ('bxb', 0.6425465941429138), ('halo', 0.6385586857795715), ('beast', 0.6233099699020386)]\n"
     ]
    }
   ],
   "source": [
    "# Podemos encontrar también nombres propios de personas célebres que están asociadas a \n",
    "# personas con las que tienen una relación, en este caso por tener similar cargo y ser\n",
    "# más o menos coetáneos\n",
    "x = model.most_similar(positive=['hulk'], topn=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647522569360,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "n8eJcfAt_orJ",
    "outputId": "35212c8a-23b7-4a71-bb01-6d6d4a1c2652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bonaparte', 0.8680719137191772), ('barbarossa', 0.7559982538223267), ('1812', 0.7390989065170288), ('xiv', 0.7368819117546082), ('1806', 0.7310059666633606)]\n"
     ]
    }
   ],
   "source": [
    "# Podemos buscar personajes históricos que en este caso, están asociados a hechos célebres, \n",
    "# años significativos, etc\n",
    "x = model.most_similar(positive=['napoleon'], topn=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1647521419601,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "BKjsujG67WJr",
    "outputId": "5c0f2715-8dab-411f-8dc7-4772f01b37ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1493', 0.8237272500991821), ('1494', 0.7913431525230408), ('1521', 0.7847018837928772), ('1519', 0.7624526619911194), ('1502', 0.7540624141693115)]\n"
     ]
    }
   ],
   "source": [
    "# Vamos a ver cómo podemos poner cifras que, en este caso, correlan con años próximos\n",
    "# O años relacionados (1492 fue la llegada de Colón a América y 1521 la conquista de \n",
    "# Tenochtitlan por Hernán Cortés)\n",
    "x = model.most_similar(positive=['1492'], topn=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1647544634518,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "WhPonhAhTPuU",
    "outputId": "f0287810-6bca-426d-d35e-43d034db4c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('largest', 0.8566276431083679), ('huge', 0.8019895553588867), ('major', 0.7904885411262512), ('big', 0.7904651761054993), ('industry', 0.7844395041465759)]\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver también que existen las palabras con sufijos \n",
    "# Y que además sus vecinas pueden dar lugar a resultados algo sorprendentes\n",
    "x = model.most_similar(positive=['biggest'], topn=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1647521687731,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "8HfA9Usi8KjL",
    "outputId": "36ae2a5b-9c06-46e2-b561-0760f4d64e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('democratic', 0.8763022422790527), ('opposition', 0.875884473323822), ('parties', 0.8393407464027405), ('coalition', 0.830763041973114), ('leader', 0.8286206126213074), ('election', 0.8271850347518921), ('candidate', 0.8257304430007935), ('elections', 0.8204044699668884), ('socialist', 0.8168104887008667), ('liberal', 0.811489462852478)]\n"
     ]
    }
   ],
   "source": [
    "# En este caso, vemos un sesgo muy fuerte de una palabra polisémica hacia un único significado\n",
    "# El sesgo de \"party\" hacia la política, se debe al entrenamiento con\n",
    "# textos de wikipedia, pero llama la atención que no esté ninguna palabra asociada con el\n",
    "# sentido de \"fiesta\"\n",
    "x = model.most_similar(positive=['party'], topn=10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIyK7-iPA_-W"
   },
   "source": [
    "## Experimentando con analogías\n",
    "A continuación vamos a jugar con las analogías para seguir profundizando en la similitud semántica que refleja nuestro word-embedding.\n",
    "\n",
    "Recordamos:\n",
    "> \"man is to king as woman is to X\"\n",
    "\n",
    "Se modela como: \n",
    ">       man - king ~ woman - x   \n",
    ">       x ~ king + woman - man \n",
    "\n",
    "Para ello volvemos a usar `most_similar` y le facilitamos en `positive` las palabras que tienen signo positivo y en `negative` las de negativo. El calculará una media de los vectores de las palabras facilitadas (usando para las palabras negativas el vector de signo contrario), y buscará la palabra más similar a al vector medio según la similitud del coseno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1647523289136,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "vua-ge_ZrLpB",
    "outputId": "4cc9c31d-c0db-4a34-b5bf-33a257d516da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.8523604273796082)]\n"
     ]
    }
   ],
   "source": [
    "x = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1647523441303,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "QekT6izduIyf",
    "outputId": "eea99d96-845a-404c-abcf-4aaa07c69375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nurse', 0.8404642939567566)]\n"
     ]
    }
   ],
   "source": [
    "# Vamos a probar con otra analogía y observar su sesgo\n",
    "x = model.most_similar(positive=['doctor', 'woman'], negative=['man'], topn=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1647523603645,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "wODr0KFpCsI2",
    "outputId": "bb0b256a-1463-4500-eda0-b5805575538d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('paris', 0.8246473670005798)]\n",
      "[('lyon', 0.8399261236190796)]\n"
     ]
    }
   ],
   "source": [
    "# Podemos pensar en analogías geográficas\n",
    "x = model.most_similar(positive=['madrid', 'france'], negative=['spain'], topn=1)\n",
    "print(x)\n",
    "x = model.most_similar(positive=['barcelona', 'france'], negative=['spain'], topn=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1647523734827,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "VtIiIHJ8vh1o",
    "outputId": "894bf178-91ed-4db5-cf0c-00f82d235688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lasagna', 0.7175830602645874)]\n"
     ]
    }
   ],
   "source": [
    "# Culinarias\n",
    "x = model.most_similar(positive=['paella', 'italy'], negative=['spain'], topn=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1647524609014,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "uuh_AEutQfNe",
    "outputId": "bc730006-2e3e-488f-bc30-662acdc6b098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spaceship', 0.7625472545623779)]\n"
     ]
    }
   ],
   "source": [
    "# De transportes\n",
    "x = model.most_similar(positive=['plane', 'space'], negative=['air'], topn=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1647544283805,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "N86co_-qFm3x",
    "outputId": "984e9eb2-cf0b-4c70-e7f9-d4df32c51667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('superboy', 0.7647712230682373)]\n"
     ]
    }
   ],
   "source": [
    "# O de cualquier cosa que se nos ocurra y ver qué sale\n",
    "x = model.most_similar(positive=['batman', 'superman'], negative=['robin'], topn=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neo--I3XHX3m"
   },
   "source": [
    "# Visualización de una word-embedding\n",
    "\n",
    "> Bloque con sangría\n",
    "\n",
    "\n",
    "\n",
    "Como ya sabemos, nuestras herramientas de visualización para mostrar datos funcionan con 2 (o a lo sumo 3) dimensiones. En este caso, al tener un espacio con 50 dimensiones, necesitamos hacer uso de técnicas matemáticas para reducir la dimensionalidad y mostrar las palabras en 2 o 3 dimensiones que tengan gran poder de caracterización.\n",
    "\n",
    "Las técnicas de reducción de la dimensionalidad lo que permiten es reducir las dimensiones originales a unas pocas que contengan la máxima información original posible. Para entendernos, generan unas pocas dimensiones que resumen las dimensiones originales. \n",
    "\n",
    "Existen muchas técnicas de reducción de la dimensionalidad, cada una con distintas filosofías. Nosotros vamos a usar la más famosa que es el *análisis de componentes principales*. En el ACP (PCA en ingles) las nuevas dimensiones (llamadas componentes) se ordenan según la cantidad de varianza de los datos originales que describen y de forma que no tengan correlación lineal entre sí. \n",
    "\n",
    "## Ajuste y transformación de los datos con ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZMaNa26NQfNo"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Creamos nuestra instancia de PCA (Principal Component Analysis)\n",
    "# Fijamos el número de componentes pricipales a 2\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Sacamos elos vectores con la siguiente funcionalidad de Gensim\n",
    "#X = model[model.vocab]   # gensim 3\n",
    "X = model.vectors         # gensim 4\n",
    "\n",
    "# Ajustamos el PCA a nuestra matriz de datos\n",
    "# y transformamos la matriz de datos de 50 dimensiones en 2 componentes principales\n",
    "wordspace2D = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8smRvbobO8HW"
   },
   "source": [
    "## Mostrando palabras en las dimensiones resultantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1647544571104,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "NrR3qTuuOk7M",
    "outputId": "2d3460bc-95e8-468b-b43e-33ac8e0e11c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD4CAYAAADLhBA1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwEElEQVR4nO3deXgUVbrH8e9LQIxsEUGWiBNcWJMQIAk7BJRFLyAiiFw3ROWi4iiODDgKMjPOwADOAOOCqIA4KNzLJqOOKAqySxLCrggMQVnUsAQBg5Dk3D/S9HRCQlpo6Cy/z/P0Q1edU1Vv1dNPXurUqXPMOYeIiMiFKhPsAEREpGRQQhERkYBQQhERkYBQQhERkYBQQhERkYAoG+wA8lOtWjUXERER7DBERIqN5OTkg8656sGMoUgmlIiICJKSkoIdhohIsWFme4Idg5q8REQkIJRQREQkIJRQRH4h5xzZ2dnBDuOimzJlCjNnzgx2GFKMKKGI+CE1NZWGDRvy6KOP0qxZMx588EEiIyOJiopizpw5ACxbtowOHTpw5513Uq9ePUaMGMGsWbOIj48nKiqKXbt2AZCWlsYdd9xBXFwccXFxrFq1Kpinlq/MzEwGDx7MfffdF+xQpBgpkg/lRYqi7du3M336dG666SamTJnCxo0bOXjwIHFxcbRv3x6AjRs38uWXX1K1alWuu+46HnroIdatW8ekSZP4+9//zsSJE3niiScYOnQobdu25ZtvvqFr1658+eWXAY83NTWVbt260aJFC1JSUqhXrx4zZ85kwoQJ/POf/yQjI4PWrVvz2muvYWYkJCTQunVrVq1aRc+ePTl27BgVK1bk6aefZvLkyUyZMoWyZcvSqFEjZs+eHfB4pfhTQhEpwMKUfYxfvJ396RlUdUepXusaWrZsydChQ+nfvz8hISHUqFGDDh06kJiYSOXKlYmLi6NWrVoAXH/99XTp0gWAqKgoli5dCsCSJUvYtm2b9zg//vgjx44do1KlSgE/h+3bt/Pmm2/Spk0bBg4cyCuvvMKQIUMYNWoUAPfeey/vv/8+PXr0ACA9PZ3PP/8cgNGjR3v3M3bsWHbv3k358uVJT08PeJxSMiihiORjYco+npm/mYzTWQB8/+NJ0k+XYWHKPs41Qnf58uW938uUKeNdLlOmDJmZmQBkZ2ezZs0aQkNDL1rs4xdvZ8+eVC6rcjVpV0QAcM899zB58mTq1q3LuHHj+Omnnzh8+DCNGzf2JpR+/frlu8/o6GjuvvtuevXqRa9evS5K3FL86RmKSD7GL97uTSZnOOcYv3g77du3Z86cOWRlZZGWlsby5cuJj4/3e99dunThpZde8i5v2LAhUGF7E+G+9AwAspzjmfmbWZiyDwAz49FHH2Xu3Lls3ryZhx9+mJMnT3q3r1ChQr77/eCDD3jsscdITk6mefPm3uQo4qvQhGJm08zsBzPbUkD5MDPb4PlsMbMsM6vqKUs1s82eMr2pKMXGfs8f5PzW33777URHR9OkSRM6derEuHHjqFmzpt/7njx5MklJSURHR9OoUSNuvfXWAl/kfeihh3I1j50xY8YMhgwZctb6vIkw68c00lO3MH7xdt59913atm0LQLVq1Th+/Dhz584tNN7s7Gy+/fZbOnbsyLhx40hPT+f48eP+nq6UIv40ec0AXgLy7T/onBsPjAcwsx7AUOfcYZ8qHZ1zBy8wTpFLqnZYqPd/+QBlq9Sg9oOvUDssFDNj/PjxjB8/Ptc2CQkJJCQkeJeXLVuWb1m1atW8PcPOlOUnKyuLN9544xfFnTcRlruqDse3fEri4pep2a4ZjzzyCEeOHCEqKoqIiAji4uIK3WdWVhb33HMPR48exTnH0KFDCQsL+0VxSelQaEJxzi03swg/99cfePeCIhIpAoZ1rZ/rGQpAaLkQhnWtD/ynB1Xbtm1Zu3YtTZo04YEHHuD555/nhx9+YNasWQA8+eSTZGRkEBoayvTp06lfvz4ZGRk88MADbNu2jYYNG5KR8Z8kULFiRZ566ikWL17Miy++yHPPPceECROIjY1l+vTpjBkzhlq1alGvXr1cz2vOyJsIMeOqrkMIDwtl3ohOALzwwgu88MILZ23rmwAh90P5lStX/uJrKKVPwJ6hmNkVQDdgns9qB3xsZslmNqiQ7QeZWZKZJaWlpQUqLJHz0qtpOGN6RxEeFooB4WGhjOkdRa+m4d46O3fu5IknnmDTpk189dVXvPPOO6xcuZIJEybw5z//mQYNGrB8+XJSUlL4wx/+wO9+9zsAXn31Va644go2bdrEs88+S3JysnefJ06cIDIyki+++MLbPAVw4MABnn/+eVatWsUnn3ySbzMY5CTC0HIhudb5JkKRiymQvbx6AKvyNHe1cc7tN7OrgU/M7Cvn3PL8NnbOTQWmAsTGxmqiewm6Xk3DcyWQhSn7aDP2M2834qtr1yEqKgqAxo0bc9NNN2FmREVFkZqaytGjR7n//vvZsWMHZsbp06cBWL58Ob/+9a+BnN5T0dHR3mOEhIRwxx13nBXLF198QUJCAtWr5wwm269fP77++ut8Y4acZyn7qUHcb6YzrGv9XOchcrEEMqHcRZ7mLufcfs+/P5jZAiAeyDehiBRl+XUjPnTSsTBlH72ahufbRXjkyJF07NiRBQsWkJqamutZiZnle5zLL7+ckJCQfMsK2iavvIlQ5FIJSJOXmVUBOgDv+ayrYGaVznwHugD59hQTKerO1Y24IEePHiU8POcP+4wZM7zr27dv733GsmXLFjZt2lTo8Vu0aMGyZcs4dOgQp0+f5v/+7//O4yxELi5/ug2/C6wB6pvZXjN70MwGm9lgn2q3Ax875074rKsBrDSzjcA64APn3EeBDF7kUjlXN+KC/Pa3v+WZZ56hTZs2ZGX9Jxk98sgjHD9+nOjoaMaNG+fXOyy1atVi9OjRtGrViptvvplmzZr98pMQucjsXG/9BktsbKzTBFtSlLQZ+1nu3lMe4WGhrPL0nhIJJjNLds7FBjMGvSkv4gf1nhIpnMbyEvFDrt5T6RnUDgtV7ymRPJRQRPyk3lMi56YmLxERCQglFBERCQglFBERCQglFBERCQglFLkgy5Yto3v37sEOQ0SKACUUEREJCCWUUuCPf/wjDRo0oHPnzvTv358JEyawYcMGWrZsSXR0NLfffjtHjhwBKHB9YmIi0dHRtGrVimHDhhEZGXnWcU6cOMHAgQOJi4ujadOmvPfee2fVEZGSSwmlhEtKSmLevHmkpKQwf/5871Sz9913H3/5y1/YtGkTUVFR/P73vz/n+gceeIApU6awZs2aAkfD/dOf/kSnTp1ITExk6dKlDBs2jBMnTuRbV0RKHr3YWAItTNnnfaObLR8SH9+R0NBQAHr06MGJEydIT0+nQ4cOANx///307duXo0eP5rs+PT2dY8eO0bp1awD++7//m/fff/+s43788ccsWrSICRMmAHDy5Em++eYbGjZseClOW0SCTAmlhMk7b8ePGaf49Kt077wd58PfAUSdc8ybN4/69TW+lUhppCavEibvvB3lr2nEsa+/4C/vb+b48eN88MEHVKhQgSuvvJIVK1YA8Pbbb9OhQweqVKmS7/orr7ySSpUqsXbtWgBmz56d77G7du3K3//+d28CSklJuZinKiJFjO5QSpi883OUr1WP0BviSZr4EL1XNCI2NpYqVarw1ltvMXjwYH766Seuu+46pk+fDlDg+jfffJOHH36YChUqkJCQQJUqVc469siRI3nyySeJjo7GOUdERES+TWMiUjJpPpQSJr95O7JPZVDn6qp88uuWtG/fnqlTp/7iCZqOHz9OxYoVARg7diwHDhxg0qRJAYtbRC6M5kORgGrdujXDutYne99Wfpj7e+/6ox+/zP7pj9OsWTPuuOOO85rt74MPPiAmJobIyEhWrFjBc889F8jQRaQEUJNXCbJ69WoAtrSvy18Ty2BA7bBQJr79jwsedr1fv37069cvAFGKSEmlO5QS5EyTVNsbqxN59WXEbH+DIzOH8NFrfyQ7O9tbZ/jw4TRv3pybb76ZdevWkZCQwHXXXceiRYuCGb6IFHOFJhQzm2ZmP5jZlgLKE8zsqJlt8HxG+ZR1M7PtZrbTzEYEMnA5t3Xr1vHiiy+yefNmdu3axfz584Gct9kTEhJITk6mUqVKPPfcc3zyyScsWLCAUaNGFbJXEZGC+dPkNQN4CZh5jjornHO5Rgg0sxDgZaAzsBdINLNFzrlt5xmr5MP3JcaM01ksTNlHGBAfH891110HQP/+/Vm5ciV9+vThsssuo1u3bgBERUVRvnx5ypUrR1RUFKmpqUE7DxEp/gq9Q3HOLQcOn8e+44Gdzrl/O+dOAbOB285jP1KAMy8x7kvPwAHOwTPzN7NyRxpmlqvumeVy5cp5v5cpU4by5ct7v2dmZl7S+EWkZAnUM5RWZrbRzP5lZo0968KBb33q7PWsy5eZDTKzJDNLSktLC1BYgZGamprvYIjBlvclRoCM01nMTvyWdevWsXv3brKzs5kzZw5t27YNUpQiUloEIqGsB37lnGsC/B1Y6Flv+dQt8KUX59xU51yscy62evXqAQir5Mv7EuMZB4//TKtWrRgxYgSRkZHUrVuX22+//RJHJyKlzQV3G3bO/ejz/UMze8XMqpFzR1LHp+o1wP4LPV6gjBw5kmrVqvHEE08A8Oyzz1KjRg3ee+89jhw5wunTp3nhhRe47bacVrqsrCwefvhhVq9eTXh4OO+99x6hoaFs2LDB+2b59ddfz7Rp07jyyisLXB9ItcNCc73EeO1TcwG4ProFn73zTL7bHD9+3Pt99OjRBZaJiPxSF3yHYmY1zdMob2bxnn0eAhKBG82srpldBtwFFJl+qQ8++CBvvfUWANnZ2cyePZt+/fqxYMEC1q9fz9KlS/nNb37jHZdqx44dPPbYY2zdupWwsDDmzZsH/PJh4ANpWNf6hJbLPZR8aLkQhnXV4IwicukVeodiZu8CCUA1M9sLPA+UA3DOTQH6AI+YWSaQAdzlcv4KZ5rZEGAxEAJMc85tvShncR4iIiK46qqrSElJ4fvvv6dp06ZUrVqVoUOHsnz5csqUKcO+ffv4/vvvAahbty4xMTEANG/enNTU1AKHey9ofaCdeVnxTC+v2mGhDOta/4JfYhQROR+FJhTnXP9Cyl8ip1txfmUfAh+eX2gXh28328uqt2Tk+JeokHWcgQMHMmvWLNLS0khOTqZcuXJERERw8uRJAG9vKICQkBAyMvJ/fnGp9WoargQiIkVCqXpTPm8325Phzflk8WI+X7WWrl27cvToUa6++mrKlSvH0qVL2bNnzzn3V9Bw7wWtFxEpyUrVWF55u9laSDkuuzaKslXCCAkJ4e6776ZHjx7ExsYSExNDgwYNCt3nLx0GXkSkpCpVw9fXHfFBrn7LzmVzYMYTXH3bCPZOHRTw44nIpbdo0SK2bdvGiBGla7SnojB8fam6Q/HtZnvq4Dekzf09ofVa8avrbghyZCISCJmZmfTs2ZOePXsGO5RSqVQllGFd63vnW7+s2rWED35T3WxFiqATJ05w5513snfvXrKyshg5ciTDhw+nX79+LF26FIB33nmHG264gQEDBlC1alVSUlJo1qwZUVFRJCUl8dJLLzFgwAAqV65MUlIS3333HePGjaNPnz5kZ2czZMgQPv/8c+rWrUt2djYDBw6kT58+QT7z4q1UPZTv1TScMb2jCA8LxYDwsFDG9I5SLymRIuajjz6idu3abNy4kS1btngHNK1cuTLr1q1jyJAhPPnkk976X3/9NUuWLOHFF188a18HDhxg5cqVvP/++95msPnz55OamsrmzZt54403WLNmzSU5r5KuVN2hgLrZihRlZ7r17/n3IQ7Oe59Dpx9l6IP9adeuHZAzcvaZf4cOHerdrm/fvoSEhOS7z169elGmTBkaNWrkfa9s5cqV9O3blzJlylCzZk06dux4kc+sdCh1CUVEiqYz3fozTmdRtmo41e/9G2v3rGfQr39D/9tzZsfwHUXb93uFChUK3K/vO2RnOiEVxc5IJUGpavISkaLLt1t/5rFDlClXnssadMBFdmf9+vUAzJkzx/tvq1atzvtYbdu2Zd68eWRnZ/P999+zbNmyC45fdIciIkWE7+jZp9NS+WHZdDDDypTlH/98hz59+vDzzz/TokULsrOzeffdd8/7WHfccQeffvopkZGR1KtXjxYtWlClSpVAnEapVqreQxGRoqvN2M9yjZ59RnhYKKtGdCIiIoKkpCSqVasWkOMdP36cihUrcujQIeLj41m1ahU1a9YMyL6DQe+hiIh4+HbrP+Niduvv3r076enpnDp1ipEjRxbrZFJUKKGISJFQ2OjZqampAT2enpsEnhKKiBQZ6tZfvKmXl4iIBIQSioiIBIQSiogE1a233kp6evo568yYMYP9+/dfmoDkvCmhiEhQffjhh4SFhZ2zzvkklMzMzAuISs5HoQnFzKaZ2Q9mtqWA8rvNbJPns9rMmviUpZrZZjPbYGZ6sUSkFBo3bhyTJ08GYOjQoXTq1AmATz/9lHvuuYeIiAgOHjxIamoqDRs25OGHH6Zx48Z06dKFjIwM5s6dS1JSEnfffTcxMTFkZGSQnJxMhw4daN68OV27duXAgQMAJCQk8Lvf/Y4OHTowadKkoJ1zaeXPHcoMoNs5yncDHZxz0cAfgal5yjs652KC/cKNiARH+/btvdNhJyUlcfz4cU6fPs3KlSu9gz6esWPHDh577DG2bt1KWFgY8+bNo0+fPsTGxjJr1iw2bNhA2bJlefzxx5k7dy7JyckMHDiQZ5991ruP9PR0Pv/8c37zm99c0vMUP7oNO+eWm1nEOcpX+yyuBa4JQFwiUoydGTV4f3oGNSuVY/eadRw7dozy5cvTrFkzkpKSWLFiBZMnT2bMmDHe7erWrUtMTAwAzZs3z/fdk+3bt7NlyxY6d+4MQFZWFrVq1fKW9+vX76KemxQs0O+hPAj8y2fZAR+bmQNec87lvXsRkRLGd9RggAPHTnOs7JUM/ePfaN26NdHR0SxdupRdu3bRsGHDXNv6jgwcEhJCRsbZQ7E452jcuHGBc5ica+RhubgC9lDezDqSk1CG+6xu45xrBtwCPGZm7c+x/SAzSzKzpLS0tECFJSKXmO+owWeUu6YRb099mfbt29OuXTumTJlCTExMriHoz6VSpUocO3YMgPr165OWluZNKKdPn2br1q2BPQk5LwFJKGYWDbwB3OacO3RmvXNuv+ffH4AFQHxB+3DOTXXOxTrnYqtXrx6IsEQkCPbnM8Bj+Wsac+rYIVq1akWNGjW4/PLLz3p+ci4DBgxg8ODBxMTEkJWVxdy5cxk+fDhNmjQhJiaG1atXF74TuWCWo8C84ddow55nKO875yLzKbsW+Ay4z/d5iplVAMo45455vn8C/ME591Fhx9NowyLFV2GjBsu5nThxgjvvvJO9e/eSlZXFyJEjGT58uHek5aSkJJ5++mmWLVvG6NGj2b17NwcOHOCTTz45BfQHWpLTKrQP6OGcO21mqcA7QEegHDAIGAPcAIx3zk0BMLNhwJ1AeWCBc+55z9//fwFLgVZAL+fcnvxi96fb8LvAGqC+me01swfNbLCZDfZUGQVcBbySp3twDWClmW0E1gEf+JNMRKR4G9a1PqHlck/HezFHDS5pPvroI2rXrs3GjRvZsmUL3bqdq5Mt7Nq1iw8++ABgJ/APYKlzLgrIAP7Lp+q3zrlWwApyeu/2ISf5/AHAzLoAN5LTkhQDNPd5TFEfmOmca1pQMgH/enn1L6T8IeChfNb/G2hy9hYiUpIVNmqwnM23V9yVp4+z78PFVB0+nO7duxfaNHjLLbdQrlw5yEkgIcCZ/7hvBiJ8qi7yWV/ROXcMOGZmJ80sDOji+aR46lUkJ8F8A+xxzq0t7Dw02rCIBJxGDfZf3l5xh8tVo0r/F/m50gGeeeYZunTpQtmyZcnOzgbg5MmTubb37RkHnHb/eY6RTe6/8T/7rP/ZZ/2ZegaMcc695rtDT5PXCX/ORUOviIgEUd5ecZnHDvEzZUksG8nTTz/N+vXriYiIIDk5GYB58+ZdrFAWAwPNrCKAmYWb2dW/ZAe6QxERCaK8veJOp6Xyw7LpHDDjT9dexauvvkpGRgYPPvggf/7zn2nRosVFicM597GZNQTWeLpzHwfuAbLOuaEPzSkvIhJEgeoVVxTmlFeTl4hIEJWkXnFq8hIRCaKS1CtOCUVEJMhKSq84NXmJiEhAKKGIiEhAKKGUMKmpqTRo0ID777+f6Oho+vTpw08//URiYiKtW7emSZMmxMfHe0duFREJFCWUEmj79u0MGjSITZs2UblyZV566SX69evHpEmT2LhxI0uWLCE0NDTYYYpICaOEUgLVqVOHNm3aAHDPPfewePFiatWqRVxcHACVK1embNmypKamEhl51gDS561ixYoB25eIFD/q5VUC+A4sV9Ud5eTp7FzllStX5ueffy5g6/OTmZlJ2bIX9+eTlZVFSEhI4RVFpEjQHUoxd2ZguX3pGTjg+x9PkvbdPsbOyBlY9N1336Vly5bs37+fxMREAI4dO0ZmZiaQ80f74YcfpnHjxnTp0oWMjAxef/114uLiaNKkCXfccQc//fQTkDPJ0VNPPUXHjh0ZPnw4u3fvplWrVsTFxTFy5EhvTI8++iiLFuUc//bbb2fgwIEAvPnmmzz33HMA9OrVi+bNm9O4cWOmTv3PzNAVK1Zk1KhRtGjRgjVr1vCPf/yD+Ph4YmJi+J//+R+ysvweBUJELjEllGIu3+lWr6rDxFdfJzo6msOHD/P4448zZ84cHn/8cZo0aULnzp29I5bu2LGDxx57jK1btxIWFsa8efPo3bs3iYmJbNy4kYYNG/Lmm2969/3111+zZMkSXnzxRZ544gkeeeQREhMTqVmzprdO+/btWbFiBQD79u1j27ZtAKxcudI7FPe0adNITk4mKSmJyZMnc+hQzkSfJ06cIDIyki+++IKrrrqKOXPmsGrVKjZs2EBISAizZs26eBdTRC6ImryKufymW8WM0I6D2TT2P3PrxMXFsXbtWm/zWNQLn1PVHeXq2nWIiYkBoHnz5qSmprJlyxaee+450tPTOX78OF27dvXup2/fvt5mqFWrVnlHPr333nsZPnw4AO3atWPixIls27aNRo0aceTIEQ4cOMCaNWuYPHkyAJMnT2bBggUAfPvtt+zYsYOrrrqKkJAQ7rjjDgA+/fRTkpOTvc9+MjIyuPrqXzT4qYhcQkooxVztsNB8B5arHXZ2L6688y58/+NJDp10LEzZR6+m4YSEhJCRkcGAAQNYuHAhTZo0YcaMGSxbtsy7jwoVKuTap2dU0lzCw8M5cuQIH330Ee3bt+fw4cP87//+LxUrVqRSpUosW7aMJUuWsGbNGq644goSEhK8d0yXX365N2E557j//vsZM2bMeV8fEbl01ORVzOUdWK5slRpcP/i1fAeWy695zDnH+MXbc607duwYtWrV4vTp0+dsYmrTpg2zZ88GOKteq1atmDhxIu3bt6ddu3ZMmDDB29x19OhRrrzySq644gq++uor1q7NfyK4m266iblz5/LDDz8AcPjwYfbsKXD2UREJMiWUYq5X03DG9I4iPCwUI2fI6zG9o/IdFyjf5rF81v/xj3+kRYsWdO7cmQYNGhR47EmTJvHyyy8TFxfH0aNHc5W1a9eOzMxMbrjhBpo1a8bhw4e9CaVbt25kZmYSHR3NyJEjadmyZb77b9SoES+88AJdunQhOjqazp07c+DAgXNdDhEJokLnQzGzaUB34Afn3FkvLVhOm8ck4FbgJ2CAc269p6ybpywEeMM5N9afoDQfysURqHkXRKToKS7zocwAup2j/BZyJrK/ERgEvApgZiHAy57yRkB/M2t0IcHKhSlJ8y6ISNFT6EN559xyzyT1BbkNmOlybnXWmlmYmdUCIoCdzrl/A5jZbE/dbRcctZyXkjTvgogUPYHo5RUOfOuzvNezLr/1F2cyZPFbSZl3QUSKnkA8lD+73yi4c6zPfydmg8wsycyS0tLSAhCWiIhcSoFIKHuBOj7L1wD7z7E+X865qc65WOdcbPXq1QMQloiIXEqBSCiLgPssR0vgqHPuAJAI3Ghmdc3sMuAuT10RESmBCn2GYmbvAglANTPbCzwPlANwzk0BPiSny/BOcroNP+ApyzSzIcBicroNT3PObb0I5yAiIkWAP728+hdS7oDHCij7kJyEIyIiJZzelBcRkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYBQQhERkYDwK6GYWTcz225mO81sRD7lw8xsg+ezxcyyzKyqpyzVzDZ7ypICfQIiIlI0lC2sgpmFAC8DnYG9QKKZLXLObTtTxzk3Hhjvqd8DGOqcO+yzm47OuYMBjVxERIoUf+5Q4oGdzrl/O+dOAbOB285Rvz/wbiCCExGR4sOfhBIOfOuzvNez7ixmdgXQDZjns9oBH5tZspkNKuggZjbIzJLMLCktLc2PsEREpCjxJ6FYPutcAXV7AKvyNHe1cc41A24BHjOz9vlt6Jyb6pyLdc7FVq9e3Y+wRESkKPEnoewF6vgsXwPsL6DuXeRp7nLO7ff8+wOwgJwmNBERKWH8SSiJwI1mVtfMLiMnaSzKW8nMqgAdgPd81lUws0pnvgNdgC2BCFxERIqWQnt5OecyzWwIsBgIAaY557aa2WBP+RRP1duBj51zJ3w2rwEsMLMzx3rHOfdRIE9ARESKBnOuoMchwRMbG+uSkvTKioiIv8ws2TkXG8wY9Ka8iIgEhBKKiIgEhBKKiIgEhBKKiIgEhBKKlGoLFy5k27ZthVcUkUIpoUippoQiEjhKKFLi9OrVi+bNm9O4cWOmTp0KQMWKFb3lc+fOZcCAAaxevZpFixYxbNgwYmJi2LVrFwkJCZzpsn7w4EEiIiIAmDFjBr169aJHjx7UrVuXl156ib/+9a80bdqUli1bcvhwzmhDCQkJDB8+nPj4eOrVq8eKFSsu7cmLBJESipQ406ZNIzk5maSkJCZPnsyhQ4fyrde6dWt69uzJ+PHj2bBhA9dff/0597tlyxbeeecd1q1bx7PPPssVV1xBSkoKrVq1YubMmd56mZmZrFu3jokTJ/L73/8+oOcmUpQV+qa8SFG3MGUf4xdvZ396BrXDQqmz+32+XPspAN9++y07duwIyHE6duxIpUqVqFSpElWqVKFHjx4AREVFsWnTJm+93r17A9C8eXNSU1MDcmyR4kAJRYq1hSn7eGb+ZjJOZwGwa9MXpKxYzPQ579Gv9Q0kJCRw8uRJPMP/AHDy5MkC91e2bFmys7PzrVe+fHnv9zJlyniXy5QpQ2Zm5ln1QkJCcq0XKenU5CXF2vjF273JBCD755+gfAUmL/+Gr776irVr1wJQo0YNvvzyS7Kzs1mwYIG3fqVKlTh27Jh3OSIiguTkZCDnWYuI+E8JRYq1/ekZuZZD6zbHZWeT+NcHGTlyJC1btgRg7NixdO/enU6dOlGrVi1v/bvuuovx48fTtGlTdu3axdNPP82rr75K69atOXhQs1aL/BIaHFKKtTZjP2NfnqQCEB4WyqoRnYIQkUhwaHBIkQs0rGt9QsuF5FoXWi6EYV3rBykikdJLCeU86GW4oqNX03DG9I4iPCwUI+fOZEzvKHo1DQ92aCKljnp5eWRmZlK2bNkCl30tXLiQ7t2706hRo0sVnpxDr6bhSiAiRUCJfIYyc+ZMJkyYgJkRHR3NnXfeyQsvvMCpU6e46qqrmDVrFjVq1GD06NHs37+f1NRUqlWrRr169XItjxkzhoEDB5KWlkb16tWZPn06e/fupXv37lSpUoUqVaowb968Ql+IExG52IrCM5QSd4eydetW/vSnP7Fq1SqqVavG4cOHMTPWrl2LmfHGG28wbtw4XnzxRQCSk5NZuXIloaGhjB49Otdyjx49uO+++7j//vuZNm0av/71r1m4cCE9e/ake/fu9OnTJ8hnKyJSdJS4hPLZZ5/Rp08fqlWrBkDVqlXZvHkz/fr148CBA5w6dYq6det66/fs2ZPQ0NB8l9esWcP8+fMBuPfee/ntb397Cc9ERKR48euhvJl1M7PtZrbTzEbkU55gZkfNbIPnM8rfbQNlYco+2oz9jNHvbWHmmj0sTNnnLXv88ccZMmQImzdv5rXXXsv1BnSFChVy7Sfvsi/ft61FRCS3QhOKmYUALwO3AI2A/maW39PoFc65GM/nD79w2wtyZviNfekZlP9VE77bsJTf/mMVC1P2cfjwYY4ePUp4eM5D27feesvv/bZu3ZrZs2cDMGvWLNq2bQuc/Xa1iIj4d4cSD+x0zv3bOXcKmA3c5uf+L2Rbv/kOv3FZ9V9RpVU/UmcO4+5b2/PUU08xevRo+vbtS7t27bxNYf6YPHky06dPJzo6mrfffptJkyYBZ79dLSIifvTyMrM+QDfn3EOe5XuBFs65IT51EoB5wF5gP/C0c26rP9v67GMQMAjg2muvbb5nzx6/T6LuiA/I7ywM2D32v/zej4hIcVUUenn5c4eS34ODvH+/1wO/cs41Af4OLPwF2+asdG6qcy7WORdbvXp1P8L6j9phob9ovYiIBJ4/CWUvUMdn+Rpy7kK8nHM/OueOe75/CJQzs2r+bBsIGn5DRCT4/EkoicCNZlbXzC4D7gIW+VYws5rm6QJlZvGe/R7yZ9tA0PAbIiLBV+h7KM65TDMbAiwGQoBpnucjgz3lU4A+wCNmlglkAHe5nIcz+W57MU5Ew2+IiARXiRx6RUSktCkuD+VFREQKVeoTSnp6Oq+88spFPcayZctYvXr1RT2GiEiwKaEooYiIBESpTygjRoxg165dxMTEMGzYMIYNG0ZkZCRRUVHMmTMHyEkIHTp04M4776RevXqMGDGCWbNmER8fT1RUlPdt+X/+85+0aNGCpk2bcvPNN/P999+TmprKlClT+Nvf/kZMTAwrVqxgz5493HTTTURHR3PTTTfxzTffBPMSiIgEhnOuyH2aN2/uLpXdu3e7xo0bO+ecmzt3rrv55ptdZmam++6771ydOnXc/v373dKlS12VKlXc/v373cmTJ13t2rXdqFGjnHPOTZw40T3xxBPOOecOHz7ssrOznXPOvf766+6pp55yzjn3/PPPu/Hjx3uP2b17dzdjxgznnHNvvvmmu+222y7R2YpISQUkuSD/7S5xw9dfiJUrV9K/f39CQkKoUaMGHTp0IDExkcqVKxMXF0etWrUAuP766+nSpQsAUVFRLF26FIC9e/cWOEy+Lw2LLyIlUalt8joz3H3bv3zGvw+eYGHKPtw5ulCXL1/e+71MmTLe5TJlypCZmQmce5j8c9Gw+CJSEpTKhOI73L1dFsqpjBM8M38z5a9pzJw5c8jKyiItLY3ly5cTHx/v934LGiY/73D3BQ2LLyJSnJXKhOI73H1IaGXKhzdi15T/4R+LlhAdHU2TJk3o1KkT48aNo2bNmn7vt6Bh8nv06MGCBQu8D+ULGhZfRKQ4K5Vvymu4exEpafSmfJBouHsRkcArlQlFw92LiAReqew2fGZU4vGLt7M/PYPaYaEM61pfoxWLiFyAUplQQMPdi4gEWqls8hIRkcBTQhERkYBQQhERkYBQQhERkYBQQhERkYDwK6GYWTcz225mO81sRD7ld5vZJs9ntZk18SlLNbPNZrbBzDRRvIhICVVot2EzCwFeBjoDe4FEM1vknNvmU2030ME5d8TMbgGmAi18yjs65w4GMG4RESli/LlDiQd2Ouf+7Zw7BcwGbvOt4Jxb7Zw74llcC1wT2DBFRKSo8yehhAPf+izv9awryIPAv3yWHfCxmSWb2aCCNjKzQWaWZGZJaWlpfoQlIiJFiT9vyuc3+1O+QxSbWUdyEorvBB9tnHP7zexq4BMz+8o5t/ysHTo3lZymMmJjY4veEMgiInJO/tyh7AXq+CxfA+zPW8nMooE3gNucc4fOrHfO7ff8+wOwgJwmNBERKWH8SSiJwI1mVtfMLgPuAhb5VjCza4H5wL3Oua991lcws0pnvgNdgC2BCl5ERIqOQpu8nHOZZjYEWAyEANOcc1vNbLCnfAowCrgKeMUzP3qmZ6KXGsACz7qywDvOuY8uypmIiEhQlcoZG0VEShrN2CgiIiWGEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEoqIiASEEkoxMGrUKJYsWRLsMEREzqnQGRsl+P7whz8EOwQRkULpDiVI/vrXvxIZGUlkZCQTJ04kNTWVhg0b8vDDD9O4cWO6dOlCRkYGAAMGDGDu3LkAfPrppzRt2pSoqCgGDhzIzz//DEBERATPP/88zZo1Iyoqiq+++ipo5yYipZMSShAkJyczffp0vvjiC9auXcvrr7/OkSNH2LFjB4899hhbt24lLCyMefPm5dru5MmTDBgwgDlz5rB582YyMzN59dVXveXVqlVj/fr1PPLII0yYMOFSn5aIlHJ+JRQz62Zm281sp5mNyKfczGyyp3yTmTXzd9vSZGHKPtqM/Ywuw18j/eoYPvk6nYoVK9K7d29WrFhB3bp1iYmJAaB58+akpqbm2n779u3UrVuXevXqAXD//fezfPlyb3nv3r0L3FZE5GIr9BmKmYUALwOdgb1Aopktcs5t86l2C3Cj59MCeBVo4ee2pcLClH08M38zGaezcA6Onczkmfmbc9UpX76893tISIi3yesM59w5j3Fm+5CQEDIzMwMUuYiIf/y5Q4kHdjrn/u2cOwXMBm7LU+c2YKbLsRYIM7Nafm5bKoxfvJ2M01kAlK/TmJ92rOXETycY+88NLFiwgHbt2hW6jwYNGpCamsrOnTsBePvtt+nQocNFjVtExF/+9PIKB771Wd5Lzl1IYXXC/dwWADMbBAwCuPbaa/0Iq3jZn/6fu43yNW+gYuRNfDfzKb4Dxj07lCuvvLLQfVx++eVMnz6dvn37kpmZSVxcHIMHD76IUYuI+M8Ka0Yxs75AV+fcQ57le4F459zjPnU+AMY451Z6lj8FfgtcV9i2+YmNjXVJSUnnf1ZFUJuxn7EvPeOs9eFhoawa0SkIEYlISWJmyc652GDG4E+T116gjs/yNcB+P+v4s22pMKxrfULLheRaF1ouhGFd6wcpIhGRwPInoSQCN5pZXTO7DLgLWJSnziLgPk9vr5bAUefcAT+3LRV6NQ1nTO8owsNCMXLuTMb0jqJX0/BghyYiEhCFPkNxzmWa2RBgMRACTHPObTWzwZ7yKcCHwK3ATuAn4IFzbXtRzqQY6NU0XAlEREqsQp+hBENJfIYiInIxFZdnKCIiIoVSQhERkYBQQhERkYBQQhERkYAokg/lzSwN2BPsOPxQDTgY7CCKAV0n/+g6FU7XqGC/cs5VD2YARTKhFBdmlhTsXhXFga6Tf3SdCqdrVLSpyUtERAJCCUVERAJCCeXCTA12AMWErpN/dJ0Kp2tUhOkZioiIBITuUEREJCCUUEREJCCUUAphZnXMbKmZfWlmW83siXzqJJjZUTPb4PmMCkaswWRml5vZOjPb6LlOv8+njpnZZDPbaWabzKxZMGINJj+vU6n/PQGYWYiZpZjZ+/mUlfrfUlHkzxTApV0m8Bvn3HozqwQkm9knzrlteeqtcM51D0J8RcXPQCfn3HEzKwesNLN/OefW+tS5BbjR82kBvEoBU0KXYP5cJ9DvCeAJ4Eugcj5l+i0VQbpDKYRz7oBzbr3n+zFyfuCa1CQPl+O4Z7Gc55O3x8dtwExP3bVAmJnVupRxBpuf16nUM7NrgP8C3iigSqn/LRVFSii/gJlFAE2BL/IpbuVpxviXmTW+tJEVDZ4mig3AD8Anzrm81ykc+NZneS+lMDn7cZ1Av6eJwG+B7ALK9VsqgpRQ/GRmFYF5wJPOuR/zFK8nZxydJsDfgYWXOLwiwTmX5ZyLAa4B4s0sMk8Vy2+zix5YEePHdSrVvycz6w784JxLPle1fNaVut9SUaOE4gdPW/c8YJZzbn7ecufcj2eaMZxzHwLlzKzaJQ6zyHDOpQPLgG55ivYCdXyWrwH2X5qoip6CrpN+T7QBeppZKjAb6GRm/8hTR7+lIkgJpRBmZsCbwJfOub8WUKempx5mFk/OdT106aIMPjOrbmZhnu+hwM3AV3mqLQLu8/TQaQkcdc4duLSRBpc/16m0/56cc884565xzkUAdwGfOefuyVOt1P+WiiL18ipcG+BeYLOn3Rvgd8C1AM65KUAf4BEzywQygLtc6RuCoBbwlpmFkPMH8H+dc++b2WDwXqcPgVuBncBPwAPBCjaI/LlO+j3lQ7+lok9Dr4iISECoyUtERAJCCUVERAJCCUVERAJCCUVERAJCCUVERAJCCUVERAJCCUVERALi/wHpbjVgaCXCbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Escribimos las palabras a representar\n",
    "words = [ \"madrid\", \"rome\", \"paris\", \"google\",\"yahoo\", \"ibm\", \"hardware\", \"pc\", \"winter\", \"summer\", \"spring\", \"tomato\", \"carrot\", \"onion\", \"autumn\"]\n",
    "\n",
    "# Recuperamos los índices de las palabras que nos interesan  \n",
    "#indexes = [model.vocab[w].index for w in words]  #gensim 3\n",
    "indexes = [model.key_to_index[w] for w in words]  #gensim 4\n",
    "\n",
    "\n",
    "# Componemos una matriz con los vectores 2D de las palabras que nos interesan\n",
    "vectors = np.vstack([wordspace2D[i,:] for i in indexes])  \n",
    "\n",
    "# Representamos los vectores en un diagrama de dispersión\n",
    "plt.scatter(vectors[:, 0], vectors[:, 1])\n",
    "\n",
    "for label, x, y in zip(words, vectors[:, 0], vectors[:, 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmglaVKb8IRz"
   },
   "source": [
    "# Uso de word-embeddings para caracterizar textos cortos\n",
    "\n",
    "Una forma sencilla de usar una word-embedding para caracterizar un texto es resumir los vectores de las palabras que aparecen en el texto en uno solo.\n",
    "\n",
    "Para ello tenemos que hacer las siguientes tareas:\n",
    "<ol>\n",
    "  <li>Partir el texto de origen en palabras (tokens).</li>\n",
    "  <li>Encontrar los vectores que se corresponden a las palabras del texto</li>\n",
    "  <li>Combinarlos en un vector resumen normalmente el vector medio, pero también también caben otras alternativas, como usar dos vectores uno con los valores mínimos observados en el texto y otro con los máximos</li>\n",
    "</ol>\n",
    "\n",
    "Esta forma de trabajar asume que en los vectores de las palabras del texto hay una prevalencia semántica que debería quedar bien reflejada por el vector (o los vectores) resumen. Si esta hipótesis se cumple, los vectores resumen caracterizarán de manera efectiva textos de temas diferentes o con sentimientos diferentes y esta estrategia puede usarse con un algoritmo de clasificación de texto. En principio, esta hipótesis es ás probable si los textos son cortos y las palabras con poder diferenciador son relativamente abundantes en los textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsDlQnEBO0UH"
   },
   "source": [
    "### 1. Partiendo el texto de origen en palabras\n",
    "\n",
    "Con el `CountVectorizer` troceamos cada texto en palabras y contamos el número de veces que aparecen. \n",
    "\n",
    "Sin embargo, puede darse el caso de que en el texto aparezcan palabras que no existen en la word-embedding. \n",
    "\n",
    "Para evitarnos tener que elminar dichas palabras, podemos pasarle al `CountVectorizer`  las palabras que queremos que reconozca  mediante el atributo `vocabulary`. \n",
    "\n",
    "En nuestro caso, queremos pasarle todas las palabras que existen en la word-embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1647609708699,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "weyvv4eCPI1h",
    "outputId": "6dc1fe74-7b5b-4009-b694-844ea7bc087b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "# En Gensim 3 se pueden obtener de la siguiente manera\n",
    "# words = list(model.vocab.keys())\n",
    "\n",
    "# En Gensim 4 se pueden obtener de la siguiente manera\n",
    "# we_vocabulary = sorted(model.key_to_index.keys(), key=lambda word: model.get_vecattr(word, \"count\"), reverse=True)\n",
    "we_vocabulary = list(model.key_to_index.keys())\n",
    "\n",
    "print(we_vocabulary[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyALfi3dj3N8"
   },
   "source": [
    "A continuación le pasamos a `CountVectorizer` nuestro corpus que contiene las siguientes frases:\n",
    "- The meal was amazing \n",
    "- The meal was disgusting\n",
    "- McDonalds hamburguers are disgusting\n",
    "- I loved the burrito and the nachos\n",
    "- I did not enjoy my pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1058,
     "status": "ok",
     "timestamp": 1647612995565,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "uyBU7bSRMskG",
    "outputId": "b45ae7bc-8fb8-4f11-a1d6-c4e15d727275"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0ba029a5a082>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Mostramos las primeras palabras del diccionario para ver que son las mismas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ponemos nuestro corpus de palabras\n",
    "corpus = [\n",
    "    'The meal was amazing.',\n",
    "    'The meal was disgusting.',\n",
    "    'McDonalds hamburguers are disgusting.',\n",
    "    'I loved the burrito and the nachos',\n",
    "    'I did not enjoy my pizza',\n",
    "]\n",
    "\n",
    "# En vocabulary le pasamos el de la word-embedding, ponemos que pase los tokens a minusculas \n",
    "# Si ponemos binary a False coge frecuencias y si está en true presencia en modo binario\n",
    "vectorizer = CountVectorizer(vocabulary = we_vocabulary, binary = False)\n",
    "\n",
    "# Si usamos la lista de palabras vacías del inglés de sklearn quitamos artículos, preposiciones, etc \n",
    "#vectorizer = CountVectorizer(vocabulary = we_vocabulary, , stop_words='english', binary = False)\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Mostramos las primeras palabras del diccionario para ver que son las mismas\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "print(tokens[0:10])\n",
    "\n",
    "\n",
    "# El array vectorizado tiene tantas filas como el corpus y \n",
    "# tantas columnas como palabras tiene la word-embedding\n",
    "# Se trata de una matriz dispersa, pero vemos que los \"the\"\n",
    "# están bien contabilizados y que se mantiene la ordenación del\n",
    "# vocabulario dado\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N19ig0L7iJFX"
   },
   "source": [
    "Lo que devuelve `vectorizer.fit_transform()` es una matriz dispersa ([scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)).\n",
    "\n",
    "Necesitamos saber para cada documento qué términos (índices) están presentes (valor distinto de cero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BQsefUjgDYmN"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "\n",
    "\n",
    "def create_doc_embedding (doc_matrix_word_freq, keyedvectors):\n",
    "    \"\"\"\n",
    "    create_doc_embedding creates a doc-embedding matrix where rows are the \n",
    "    documents and columns are the dimensions of the word-embedding. Each row r\n",
    "    summarizes the vectors of the word-embeddings present in document r\n",
    "\n",
    "    :param doc_matrix_word_freq: frequency matrix of the terms in each document\n",
    "            It admits binary values, frequencies or weights.\n",
    "            It admits the output of vectorizer.fit_transform\n",
    "    :param keyedvectors: keyedvectors object from Gensim which has the \n",
    "            word-embedding information\n",
    "    :return: returns the doc-embedding matrix that summarizes the \n",
    "             vector-embeddings of the words present in each document\n",
    "    \"\"\" \n",
    "\n",
    "    \n",
    "    num_docs, num_words= doc_matrix_word_freq.get_shape()\n",
    "\n",
    "    # Creamos la matriz de documentos con tantas columnas como dimensiones tiene el we \n",
    "    doc_embed_matrix = np.zeros([num_docs,keyedvectors.vector_size])\n",
    "\n",
    "\n",
    "    # Obtenemos los documentos\n",
    "    docs, col, val = find(doc_matrix_word_freq)\n",
    "    docs = np.unique(docs)\n",
    "\n",
    "    # Para cada documento (esto se podrá hacer de forma más pythonica)\n",
    "    for doc in docs:\n",
    "      # Obtenemos las palabras presentes en el documento y su frecuencia\n",
    "      row, words, frecs = find(doc_matrix_word_freq[doc,:])\n",
    "\n",
    "      # Calculamos el vector medio para el documento\n",
    "      # Para ello recuperamos los vectores del we presentes en el documento y \n",
    "      # los promediamos teniendo en cuenta su frecuencia\n",
    "      count = 0\n",
    "      for w,f in zip(words,frecs):\n",
    "        doc_embed_matrix[doc,:] = doc_embed_matrix[doc,:] + (keyedvectors[w] * f)\n",
    "        count = count +1\n",
    "      doc_embed_matrix[doc,:] = doc_embed_matrix[doc,:]/count\n",
    "    \n",
    "      \n",
    "    return doc_embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-THYKV0GShhv"
   },
   "outputs": [],
   "source": [
    "doc_embed_matrix = create_doc_embedding(X,model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boEeBsW8KIho"
   },
   "source": [
    "\n",
    "\n",
    "Una ventaja de usar word-embeddings para caracterizar textos en tareas de clasificación es que **el algoritmo de clasificación no necesitará haber visto determinadas palabras en entrenamiento para \"reconocer\" su significado** (siempre que éstas existan en la word-embedding).\n",
    "\n",
    "De esta forma, supongamos la frase:\n",
    "- I had a marvelous snack\n",
    "\n",
    "Si miramos palabra por palabra, no  hay ninguna frase realmente similar en nuestro corpus. Quizás las últimas porque tienen la palabra 'I'.\n",
    "\n",
    "Sin embargo, gracias a que el word-embedding captura la similitud de los términos 'snack' y 'meal', y la de los términos 'amazing' y 'marvelous', la distancia del coseno logra encontrar la frase con un sentido más similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1647613007073,
     "user": {
      "displayName": "JAVIER ARROYO GALLARDO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02428824756029313673"
     },
     "user_tz": -60
    },
    "id": "iOpXTqTnK6xm",
    "outputId": "209da722-c5f4-4a1d-9774-f12ac392fdf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86611397 0.85057326 0.67385038 0.82627872 0.83924998]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = vectorizer.fit_transform(['I had a marvelous snack'])\n",
    "\n",
    "embed_query = create_doc_embedding(query,model)\n",
    "\n",
    "simil = cosine_similarity(embed_query, doc_embed_matrix)\n",
    "\n",
    "print(simil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOcvh0qLL5kV"
   },
   "source": [
    "Podemos ver que gracias a las word-embeddings logra discernir correctamente y asociar como  frase más similar la primera en lugar de la segunda.\n",
    "\n",
    "Sin embargo, podemos intuir también lo tosco de esta aproximación que resume un documento en base a sus palabras, y lo susceptible al ruido (palabras vacías o palabras sin sentimiento positivo/negativo) que es. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Introduccion a word-embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
